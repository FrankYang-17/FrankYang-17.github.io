---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# üë®‚Äçüéì About Me
I am a second-year Ph.D. student at Peking University, advised by Prof. Xuejun Yang and Prof. Wenjing Yang. I earned my B.S. degree at China University of Geosciences in 2023.

My primary research interest focus on **Foundation Models for Multimodal Learning**. I am also interested in **Causal Inference** and **Reinforcement Learning**. My overarching research goal is to build reliable and generalizable multimodal intelligence, with a focus on developing principled methods that integrate vision, language, and structured reasoning under real-world conditions.

Currently I am working on **Efficient Pretraining and Fine-tuning of Multimodal Large Language Models** and **Unified Models (Any-to-Any)**.

*If you are interested in partnering on research projects, offering internship opportunities or exchange programs, I would be thrilled to connect with you.* üòÑ


# üìù Publications
<span style="font-size: 90%;">*\* Equal Contribution, ‚Ä† Corresponding Author, ‚Ä° Project Leader, # Core Contributor*</span>
- [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079?)<br><span style="font-size: 80%;">*Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, **Yang Shi**, Sihan Yang, Pengfei Wan, Qiang Liu‚Ä†, Liang Wang, Tieniu Tan*</span>
- [MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios](https://mme-videoocr.github.io/)<br><span style="font-size: 80%;">***Yang Shi#**, Huanqian Wang#, Wulin Xie#, Huanyao Zhang#, Lijie Zhao#, YiFan Zhang#‚Ä†, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yuanxing Zhang‚Ä°, Pengfei Wan, Haotian Wang‚Ä†, Wenjing Yang‚Ä†*</span>
- [Mavors: Multi-granularity Video Representation for Multimodal Large Language Model](https://mavors-mllm.github.io/) [ACM MM 2025]<br><span style="font-size: 80%;">***Yang Shi**\*, Jiaheng Liu\*, Yushuo Guan\*, Zhenhua Wu, Yuanxing Zhang‚Ä†, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, Bohan Zeng, Wentao Zhang, Fuzheng Zhang, Wenjing Yang, Di Zhang*</span>
- [MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models](https://arxiv.org/abs/2504.03641)<br><span style="font-size: 80%;">*Wulin Xie\*, Yi-Fan Zhang\*‚Ä°, Chaoyou Fu, **Yang Shi**, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, Tieniu Tan*</span>
- [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391) [ICML 2025]<br><span style="font-size: 80%;">*Yi-Fan Zhang‚Ä°, Tao Yu, Haochen Tian, Chaoyou Fu‚Ä†, Peiyan Li, Jianshu Zeng, Wulin Xie, **Yang Shi**, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen‚Ä†, Fan Yang, Zhang Zhang‚Ä†, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan*</span>
- [EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents](https://arxiv.org/abs/2501.11858v1)<br><span style="font-size: 80%;">*Zhili Cheng‚Ä°, Yuge Tu\#, Ran Li\#, Shiqi Dai\#, Jinyi Hu\#‚Ä°, Shengding Hu, Jiahao Li, **Yang Shi**, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun‚Ä†*</span>
- [Debiasing Multimodal Large Language Models via Penalization of Language Priors](https://arxiv.org/abs/2403.05262) [ACM MM 2025]<br><span style="font-size: 80%;">*YiFan Zhang\*, **Yang Shi\***, Weichen Yu, Qingsong Wen‚Ä†, Xue Wang, Wenjing Yang, Zhang Zhang, Liang Wang, Rong Jin*</span>


# üë®‚Äçüíª Work Experience
- Research Intern at *Kling AI*, **Kuaishou Technology**, 2025.02 - Present
- Research Intern at *THUNLP*, **Tsinghua University**, 2023.11 - 2025.02


# üìö Education
- **Ph.D.** School of Computer Science, **Peking University**, 2023 - Present
- **B.S.** School of Computer Science, **China University of Geosciences**, 2019 - 2023


# üåü Honors & Awards
- Ruiming Alumni Scholarship, **1‚Ä∞** , 2021
- China National Scholarship, **0.2%** , 2020