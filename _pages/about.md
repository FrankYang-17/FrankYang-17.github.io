---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# üë®‚Äçüéì About Me
I am a third-year Ph.D. student at Peking University, advised by Prof. Xuejun Yang and Prof. Wenjing Yang. I earned my B.S. degree at China University of Geosciences in 2023. Prior to that, I served for two years in the People‚Äôs Liberation Army.

My primary research interest focus on **Foundation Models for Multimodal Learning**. I am also interested in **Causal Inference** and **Reinforcement Learning**. My overarching research goal is to build reliable and generalizable multimodal intelligence, with a focus on developing principled methods that integrate vision, language, and structured reasoning under real-world conditions.

Currently I am working on **Efficient Pre-training and Fine-tuning of Multimodal Large Language Models**.

*I am actively seeking research discussions and collaboration opportunities, so feel free to contact me!* üòÑ


# üìù Publications
<span style="font-size: 90%;">*\* Equal Contribution, ‚Ä† Corresponding Author, ‚Ä° Project Leader, # Core Contributor*</span>
- [Towards Efficient Multimodal Large Language Models: A Survey on Token Compression](https://www.techrxiv.org/doi/full/10.36227/techrxiv.176823010.07236701/v1)<br><span style="font-size: 80%;">*Linli Yao\*, Long Xing\*, **Yang Shi\***, Sida Li, Yuanxin Liu, Yuhao Dong, Yi-Fan Zhang, Lei Li, Qingxiu Dong, Xiaoyi Dong, Qidong Huang, Haotian Wang, Feng Wu, Yuanxing Zhang, Pengfei Wan, Zhouchen Lin‚Ä†, Xu Sun‚Ä†*</span>
- [GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models](https://arxiv.org/abs/2512.15560)<br><span style="font-size: 80%;">*Bozhou Li, Sihan Yang, Yushuo Guan, Ruichuan An, Xinlong Chen, **Yang Shi**, Pengfei Wan, Wentao Zhang, Yuanxing zhang*</span>
- [The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss](https://arxiv.org/abs/2512.08374)<br><span style="font-size: 80%;">*Bozhou Li, Xinda Xue, Sihan Yang, **Yang Shi**, Xinlong Chen, Yushuo Guan, Yuanxing Zhang, Wentao Zhang*</span>
- [Hybrid Attribution Priors for Explainable and Robust Model Training](https://arxiv.org/abs/2512.14719)<br><span style="font-size: 80%;">*Zhuoran Zhang, Feng Zhang, Shangyuan Li, **Yang Shi**, Yuanxing Zhang, Wei Chen, Tengjiao Wang, Kam-Fai Wong*</span>
- [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)<br><span style="font-size: 80%;">*Yuran Wang\*, Bohan Zeng\*, Chengzhuo Tong, Wenxuan Liu, **Yang Shi**, Xiaochen Ma, Hao Liang, Yuanxing Zhang, Wentao Zhang‚Ä†*</span>
- [Monet: Reasoning in Latent Visual Space Beyond Images and Language](https://arxiv.org/abs/2511.21395)<br><span style="font-size: 80%;">*Qixun Wang, **Yang Shi**, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying‚Ä†, Yisen Wang‚Ä†*</span>
- [A Survey of Unified Multimodal Understanding and Generation: Advances and Challenges](https://www.techrxiv.org/users/993777/articles/1355509-a-survey-of-unified-multimodal-understanding-and-generation-advances-and-challenges)<br><span style="font-size: 80%;">*Yan Yang\*, Haochen Tian\*, **Yang Shi\***, Wulin Xie\*, Yi-Fan Zhang‚Ä†, Yuhao Dong, Yibo Hu, Liang Wang, Ran He, Caifeng Shan, Chaoyou Fu‚Ä†, Tieniu Tan*</span>
- [Detecting Unobserved Confounders: A Kernelized Regression Approach]() [**AAAI 2026**]<br><span style="font-size: 80%;">*Yikai Chen, Yunxin Mao, Hao Zou, Chunyuan Zheng, Shanzhi Gu, Haotian Wang, Shixuan Liu, **Yang Shi**, Kun Kuang, Wenjing Yang*</span>
- [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)<br><span style="font-size: 80%;">*Zhuoran Zhang, Tengyue Wang, Xilin Gong, **Yang Shi**, Haotian Wang, Di Wang, Lijie Hu*</span>
- [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://www.arxiv.org/abs/2510.14265)<br><span style="font-size: 80%;">*Xukai Wang\*, Xuanbo Liu\*, Mingrui Chen\*, Haitian Zhong\*, Xuanlin Yang\*, Bohan Zeng\*, Jinbo Hu\*, Hao Liang, Junbo Niu, Xuchen Li, Ruitao Wu, Ruichuan An, **Yang Shi**, Liu Liu, Xu-Yao Zhang, Qiang Liu, Zhouchen Lin, Wentao Zhang‚Ä†, Bin Dong‚Ä†*</span>
- [AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration](https://arxiv.org/abs/2510.10395)<br><span style="font-size: 80%;">*Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, **Yang Shi**, Bozhou Li, Yuanxing Zhang, Qiang Liu‚Ä†, Pengfei Wan, Liang Wang, Tieniu Tan*</span>
- [RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark](https://arxiv.org/abs/2509.24897)<br><span style="font-size: 80%;">***Yang Shi#**, Yuhao Dong#‚Ä°, Yue Ding#, Yuran Wang#, Xuanyu Zhu#, Sheng Zhou#, Wenting Liu#, Haochen Tian#, Rundong Wang#, Huanqian Wang, Zuyan Liu, Bohan Zeng, Ruizhe Chen, Qixun Wang, Zhuoran Zhang, Xinlong Chen, Chengzhuo Tong, Bozhou Li, Chaoyou Fu, Qiang Liu, Haotian Wang‚Ä†, Wenjing Yang, Yuanxing Zhang‚Ä†, Pengfei Wan, Yi-Fan Zhang‚Ä†, Ziwei Liu‚Ä†*</span>
- [OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing](https://arxiv.org/abs/2509.24900)<br><span style="font-size: 80%;">*Zhihong Chen\*, Xuehai Bai\*, **Yang Shi\***, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang‚Ä†, Pengfei Wan, Yi-Fan Zhang‚Ä†‚Ä°*</span>
- [BaseReward: A Strong Baseline for Multimodal Reward Model](https://arxiv.org/abs/2509.16127)<br><span style="font-size: 80%;">*Yi-Fan Zhang\*, Haihua Yang\*‚Ä°, Huanyu Zhang, **Yang Shi**, Zezhou Chen, Haochen Tian, Chaoyou Fu‚Ä†, Kai Wu, Bo Cui, Xu Wang, Jianfei Pan, Haotian Wang, Zhang Zhang‚Ä†, Liang Wang*</span>
- [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079?)<br><span style="font-size: 80%;">*Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, **Yang Shi**, Sihan Yang, Pengfei Wan, Qiang Liu‚Ä†, Liang Wang, Tieniu Tan*</span>
- [MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios](https://mme-videoocr.github.io/) [**NeurIPS 2025**]<br><span style="font-size: 80%;">***Yang Shi#**, Huanqian Wang#, Wulin Xie#, Huanyao Zhang#, Lijie Zhao#, YiFan Zhang#‚Ä†, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yuanxing Zhang‚Ä°, Pengfei Wan, Haotian Wang‚Ä†, Wenjing Yang‚Ä†*</span>
- [Mavors: Multi-granularity Video Representation for Multimodal Large Language Model](https://mavors-mllm.github.io/) [**ACM MM 2025**]<br><span style="font-size: 80%;">***Yang Shi**\*, Jiaheng Liu\*, Yushuo Guan\*, Zhenhua Wu, Yuanxing Zhang‚Ä†, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, Bohan Zeng, Wentao Zhang, Fuzheng Zhang, Wenjing Yang, Di Zhang*</span>
- [MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models](https://arxiv.org/abs/2504.03641)<br><span style="font-size: 80%;">*Wulin Xie\*, Yi-Fan Zhang\*‚Ä°, Chaoyou Fu, **Yang Shi**, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, Tieniu Tan*</span>
- [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391) [**ICML 2025**]<br><span style="font-size: 80%;">*Yi-Fan Zhang‚Ä°, Tao Yu, Haochen Tian, Chaoyou Fu‚Ä†, Peiyan Li, Jianshu Zeng, Wulin Xie, **Yang Shi**, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen‚Ä†, Fan Yang, Zhang Zhang‚Ä†, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan*</span>
- [EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents](https://arxiv.org/abs/2501.11858v1)<br><span style="font-size: 80%;">*Zhili Cheng‚Ä°, Yuge Tu\#, Ran Li\#, Shiqi Dai\#, Jinyi Hu\#‚Ä°, Shengding Hu, Jiahao Li, **Yang Shi**, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun‚Ä†*</span>
- [Debiasing Multimodal Large Language Models via Penalization of Language Priors](https://arxiv.org/abs/2403.05262) [**ACM MM 2025**]<br><span style="font-size: 80%;">*YiFan Zhang\*, **Yang Shi\***, Weichen Yu, Qingsong Wen‚Ä†, Xue Wang, Wenjing Yang, Zhang Zhang, Liang Wang, Rong Jin*</span>


# üë®‚Äçüíª Work Experience
- Research Intern at *Kling AI*, **Kuaishou Technology**, 2025.02 - Present
- Research Intern at *THUNLP*, **Tsinghua University**, 2023.11 - 2025.02


# üìö Education
- **Ph.D.** School of Computer Science, **Peking University**, 2023 - Present
- **B.S.** School of Computer Science, **China University of Geosciences**, 2019 - 2023


# üåü Honors & Awards
- Ruiming Alumni Scholarship, **1‚Ä∞** , 2021
- China National Scholarship, **0.2%** , 2020